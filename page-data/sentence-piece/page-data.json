{"componentChunkName":"component---src-templates-blog-post-js","path":"/sentence-piece/","result":{"data":{"site":{"siteMetadata":{"title":"The Papers not read","author":"The Chuong Chu"}},"markdownRemark":{"id":"3889001f-0907-5af2-a8ae-b1dba371279e","html":"<h2>1. Introduction</h2>\n<h3>1.1. Moses:</h3>\n<pre><code>- Advantage: Useful pre- and postprocessor\n- Disadvantage: Hand-crafted and language dependent\n</code></pre>\n<h3>1.2. Sentencepiece</h3>\n<pre><code>- 2 algorithms: BPE and unigram\n- Not language-dependent\n</code></pre>\n<h2>2. SentencePiece overview</h2>\n<ul>\n<li>\n<p>4 main components:</p>\n<ul>\n<li>Normalizer - normalize semantically equivalent Unicode characters into canonical forms</li>\n<li>Trainer, Encoder, Decoder - as normal</li>\n</ul>\n</li>\n</ul>\n<h2>3. Library Design</h2>\n<h3>3.1 Lossless Tokenization</h3>\n<pre><code>- Decoder(Encoder(text))= text \n    example: Encode(\"Hello world.\") = Hello _wor ld . (no space between \"world\" and \".\")\n</code></pre>\n<h3>3.2 Effient subword training and segmentation</h3>\n<pre><code>- BPE segmentation: Merged symbols are managed by a binary heap (priority queue)\n    -> O(Nlog(N))\n- Unigram segmentation: O(N)\n</code></pre>\n<h3>3.3 Vocab id management</h3>\n<pre><code>spm specifies the final size of vocab\n</code></pre>\n<h3>3.4 Customizable character normalization</h3>\n<pre><code>- Unicode standard Normalization Froms (NFC, NFKC) used widely in NLP\n</code></pre>\n<h3>3.5 Self-contained models</h3>\n<h3>3.6 Library API for on-the-fly processing</h3>\n<h2>4. Experiments</h2>\n<ul>\n<li>Dataset: KFTT (440k, 1166, 1160 sentences)</li>\n<li>Model: GNMT (Eng-Ja)</li>\n<li>Compare to: Moses - English, Kytea - Ja</li>\n<li>Metric : case-sensitive BLEU </li>\n</ul>","timeToRead":1,"frontmatter":{"title":"SentencePiece: A simple and language independent subword tokenizer","date":"April 06, 2021","spoiler":null,"cta":null},"fields":{"slug":"/sentence-piece/","langKey":"en"}}},"pageContext":{"slug":"/sentence-piece/","previous":{"fields":{"slug":"/machine-reading-comprehension/","langKey":"en","directoryName":"machine-reading-comprehension","maybeAbsoluteLinks":[]},"frontmatter":{"title":"Implementation of Machine Reading Comprehension"}},"next":{"fields":{"slug":"/deep-learning-book/","langKey":"en","directoryName":"deep-learning-book","maybeAbsoluteLinks":[]},"frontmatter":{"title":"Deep Learning in a nutshell - A summary"}},"translations":[],"translatedLinks":[]}},"staticQueryHashes":["336482444"]}